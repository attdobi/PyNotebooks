{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Predict the type of question with Doc2Vec and Word2vec</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['shuffle', 'sample']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence, TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "import re\n",
    "# random\n",
    "from random import shuffle,sample\n",
    "\n",
    "# classifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "import psycopg2\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Connect to the DB </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"host=localhost port=5432 dbname=amazon user=postgres password=darkmatter\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3535,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cur.execute(\"SELECT question,qestion_type,qestion_type_bow,qestion_type_human from training \\\n",
    "WHERE qestion_type_human  = 'yes/no' and (name='Attila' or name='Ruth' ) ;\")\n",
    "result=cur.fetchall()\n",
    "Qyn=[val[0] for val in result]\n",
    "Qyn_Type_data=[val[1] for val in result]\n",
    "Qyn_Type_bow=[val[2] for val in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3536,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cur.execute(\"SELECT question,qestion_type,qestion_type_bow,qestion_type_human from training \\\n",
    "WHERE qestion_type_human  = 'open-ended' and (name='Attila' or name='Ruth') ;\")\n",
    "result=cur.fetchall()\n",
    "Qoe=[val[0] for val in result]\n",
    "Qoe_Type_data=[val[1] for val in result]\n",
    "Qoe_Type_bow=[val[2] for val in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7176669484361792"
      ]
     },
     "execution_count": 3285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Qyn)/(len(Qyn)+len(Qoe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> setup training </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3537,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_line(sentence):\n",
    "    filter_text=' '.join(re.findall(\"[a-z']+\", sentence.lower())) #removed ?\n",
    "    #return nltk.word_tokenize(filter_text)\n",
    "    return filter_text.replace('?',' ? ').split()\n",
    "stoplist = set('for a of the and to in rt'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> turn questions into bag of words. sample 1/2 for training </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3538,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qs_yn= [[word for word in process_line(sentence) if word not in stoplist] for sentence in Qyn ]\n",
    "#shuffle and take the first 1/2 for training\n",
    "shuffle(qs_yn)\n",
    "qs_yn_sample=qs_yn[:int(len(qs_yn)/2)]\n",
    "\n",
    "qs_oe= [[word for word in process_line(sentence) if word not in stoplist] for sentence in Qoe ]\n",
    "#shuffle and take the first 1/2 for training, can also do random sampling\n",
    "shuffle(qs_oe)\n",
    "qs_oe_sample=qs_oe[:int(len(qs_oe)/2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> setup labeles </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1040,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs=[]\n",
    "for i,words in enumerate(qs_yn):\n",
    "    docs.append(TaggedDocument(words[:3],['YN_'+str(i)]))\n",
    "for i,words in enumerate(qs_oe):\n",
    "    docs.append(TaggedDocument(words[:3],['OE_'+str(i)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Train doc2vec </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5673"
      ]
     },
     "execution_count": 1108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Doc2Vec(min_count=1, window=10, size=4, sample=1e-4, negative=2, workers=2)\n",
    "model.build_vocab(docs)\n",
    "model.train(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model.docvecs['OE_7']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> setup logistic regresion training </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_yn_arrays = np.zeros((len(qs_oe_sample), 4))\n",
    "all_oe_arrays = np.zeros((len(qs_oe_sample), 4))\n",
    "all_yn_labels = np.zeros(len(qs_oe_sample))\n",
    "all_oe_labels = np.zeros(len(qs_oe_sample))\n",
    "\n",
    "#setup training for Y/N questions\n",
    "\n",
    "## KEEP same size of YN and OE when training #####\n",
    "\n",
    "for i in range(len(qs_oe_sample)):\n",
    "    all_yn_arrays[i] = model.docvecs['YN_'+str(i)]\n",
    "    all_yn_labels[i] = 1\n",
    "\n",
    "#setup training for open-ended questions\n",
    "for ii in range(len(qs_oe_sample)):\n",
    "    all_oe_arrays[ii] = model.docvecs['OE_'+str(ii)]\n",
    "    all_oe_labels[ii] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Setup test labels for logistic regresion </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get length\n",
    "Nyn=len(all_oe_arrays)\n",
    "Noe=len(all_oe_arrays)\n",
    "\n",
    "ratio=0.5\n",
    "\n",
    "train_arrays = np.vstack((all_yn_arrays[:Nyn*ratio],all_oe_arrays[:Noe*ratio]))\n",
    "train_labels = np.hstack((all_yn_labels[:Nyn*ratio],all_oe_labels[:Noe*ratio]))\n",
    "\n",
    "test_arrays = np.vstack((all_yn_arrays[Nyn*ratio:],all_oe_arrays[Noe*ratio:]))\n",
    "test_labels = np.hstack((all_yn_labels[Nyn*ratio:],all_oe_labels[Noe*ratio:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Run Logistic Resgresion Fit </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 1112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(train_arrays, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Test Logistice Regrsion on training set (used to create the doc2vec model) </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45555555555555555"
      ]
     },
     "execution_count": 1113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(test_arrays, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Test on data not used to train the word2vec model </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Y/N </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "prediction=[classifier.predict(model.infer_vector(line_arr[:3])) for line_arr in qs_yn[-100:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.53])"
      ]
     },
     "execution_count": 1115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([val==1 for val in prediction])/len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#[(val1,val2) for val1,val2 in zip(prediction,qs_yn[-200:])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> OE </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.49])"
      ]
     },
     "execution_count": 1117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_oe=[classifier.predict(model.infer_vector(line_arr[:3])) for line_arr in qs_oe[-100:]]\n",
    "sum([val==0 for val in prediction_oe])/len(prediction_oe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Test on a sentence </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.])"
      ]
     },
     "execution_count": 742,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_arr=\"can I use this with an iphone\".split()\n",
    "classifier.predict(model.infer_vector(lin_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.])"
      ]
     },
     "execution_count": 743,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_arr=\"how do you plug this is\".split()\n",
    "classifier.predict(model.infer_vector(lin_arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Use Bigrams and first word of a sentence for logistic regresion </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "QmodelB=models.Word2Vec.load('/home/ubuntu/TallLabs/models/QmodelB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3491,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fail\n",
      "fail\n",
      "fail ['is', 'that', 'harcore', 'or', 'usual', 'nitro', 'tech']\n",
      "fail ['is', 'this', 'shibari', 'my', 'wand', 'or', 'new', 'generation', 'hitachi', 'wand']\n",
      "fail ['what', 'warrebnty', 'does', 'it', 'have']\n"
     ]
    }
   ],
   "source": [
    "#use the length of qs_oe_sample to get a 50/50 sample\n",
    "#only look at questions with more than 3 word inputs\n",
    "#shuffle and take the first 1/2 for training\n",
    "qs_yn=[val for val in qs_yn if len(val)>2]\n",
    "shuffle(qs_yn)\n",
    "qs_yn_sample=qs_yn[:int(len(qs_yn)/2)]\n",
    "\n",
    "#shuffle and take the first 1/2 for training, can also do random sampling\n",
    "qs_oe=[val for val in qs_oe if len(val)>2]\n",
    "shuffle(qs_oe)\n",
    "qs_oe_sample=qs_oe[:int(len(qs_oe)/2)]\n",
    "\n",
    "all_yn_arrays=[]\n",
    "all_oe_arrays=[]\n",
    "all_yn_labels=[]\n",
    "all_oe_labels=[]\n",
    "\n",
    "all_yn_arrays2=[]\n",
    "all_oe_arrays2=[]\n",
    "all_yn_labels2=[]\n",
    "all_oe_labels2=[]\n",
    "\n",
    "all_yn_arrays3=[]\n",
    "all_oe_arrays3=[]\n",
    "all_yn_labels3=[]\n",
    "all_oe_labels3=[]\n",
    "\n",
    "all_yn_arrays4=[]\n",
    "all_oe_arrays4=[]\n",
    "all_yn_labels4=[]\n",
    "all_oe_labels4=[]\n",
    "\n",
    "all_yn_labels=[]\n",
    "all_oe_labels=[]\n",
    "len_yn=[]\n",
    "len_oe=[]\n",
    "\n",
    "#setup training for Y/N questions ... only tranin on the first word!\n",
    "for i in range(len(qs_oe_sample)):\n",
    "    if(qs_yn_sample[i]==[]):\n",
    "        qs_yn_sample[i]=['']\n",
    "    if (qs_yn_sample[i][0] in QmodelB)&(qs_yn_sample[i][1] in QmodelB)&(qs_yn_sample[i][2] in QmodelB):\n",
    "        #print(qs_yn_sample[i][2])\n",
    "        all_yn_arrays.append(QmodelB[qs_yn_sample[i][0]])\n",
    "        all_yn_arrays2.append(QmodelB[qs_yn_sample[i][1]])\n",
    "        all_yn_arrays3.append(QmodelB[qs_yn_sample[i][2]])\n",
    "        #all_yn_arrays4.append(QmodelB[qs_yn_sample[i][3]])\n",
    "        len_yn.append(len(qs_yn_sample[i]))\n",
    "        all_yn_labels.append(1)\n",
    "    else:\n",
    "        print('fail')\n",
    "\n",
    "#setup training for open-ended questions\n",
    "for ii in range(len(qs_oe_sample)):\n",
    "    if(qs_oe_sample[ii]==[]):\n",
    "        qs_oe_sample[ii]=['']\n",
    "    if (qs_oe_sample[ii][0] in QmodelB)&(qs_oe_sample[ii][1] in QmodelB)&(qs_oe_sample[ii][2] in QmodelB):\n",
    "        #print(qs_oe_sample[ii][2])\n",
    "        all_oe_arrays.append(QmodelB[qs_oe_sample[ii][0]])\n",
    "        all_oe_arrays2.append(QmodelB[qs_oe_sample[ii][1]])\n",
    "        all_oe_arrays3.append(QmodelB[qs_oe_sample[ii][2]])\n",
    "        #all_oe_arrays4.append(QmodelB[qs_oe_sample[ii][3]])\n",
    "        len_oe.append(len(qs_oe_sample[ii]))\n",
    "        all_oe_labels.append(0)\n",
    "    else:\n",
    "        print('fail',qs_oe_sample[ii])\n",
    "    \n",
    "#get length\n",
    "Noe=len(all_oe_arrays)\n",
    "ratio=0.99\n",
    "N=int(Noe*ratio)\n",
    "\n",
    "len_yn=np.array(len_yn)\n",
    "len_oe=np.array(len_oe)\n",
    "\n",
    "train_arrays = np.vstack((all_yn_arrays[:N],all_oe_arrays[:N]))\n",
    "len_train=np.hstack((len_yn[:N],len_oe[:N]))\n",
    "len_train=len_train.reshape(len(len_train),1)\n",
    "train_labels = np.hstack((all_yn_labels[:N],all_oe_labels[:N]))\n",
    "train_arrays2 = np.vstack((all_yn_arrays2[:N],all_oe_arrays2[:N]))\n",
    "train_arrays3 = np.vstack((all_yn_arrays3[:N],all_oe_arrays3[:N]))\n",
    "train_arrays4 = np.vstack((all_yn_arrays4[:N],all_oe_arrays4[:N]))\n",
    "\n",
    "\n",
    "test_arrays = np.vstack((all_yn_arrays[N:],all_oe_arrays[N:]))\n",
    "len_test=np.hstack((len_yn[N:],len_oe[N:]))\n",
    "len_test=len_test.reshape(len(len_test),1)\n",
    "test_labels = np.hstack((all_yn_labels[N:],all_oe_labels[N:]))\n",
    "test_arrays2 = np.vstack((all_yn_arrays2[N:],all_oe_arrays2[N:]))\n",
    "test_arrays3 = np.vstack((all_yn_arrays3[N:],all_oe_arrays3[N:]))\n",
    "test_arrays4 = np.vstack((all_yn_arrays4[N:],all_oe_arrays4[N:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3459,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot length of words\n",
    "#bins=range(25)\n",
    "#plt.hist(len_yn,bins=bins,normed=1)\n",
    "#plt.hist(len_oe,color='red',alpha=0.5,bins=bins,normed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> USA PCA to reduce to 2 principle components </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3444,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pca_train_vec=np.vstack((train_arrays,test_arrays))\n",
    "#pca_train_vec2=np.vstack((train_arrays2,test_arrays2))\n",
    "#pca_train_vec3=np.vstack((train_arrays3,test_arrays3))\n",
    "#pca_train_vec4=np.vstack((train_arrays4,test_arrays4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3479,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n"
     ]
    }
   ],
   "source": [
    "all_first_three_words=[[word for word in words][:3] for words in qs_yn+qs_oe]\n",
    "all_first_three_wordVec =[]\n",
    "for words in all_first_three_words:\n",
    "    for word in words:\n",
    "        try:\n",
    "            all_first_three_wordVec.append(QmodelB[word])\n",
    "        except KeyError:\n",
    "            print('fail')\n",
    "all_words=[QmodelB[word] for word in QmodelB.index2word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3492,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1, whiten=True)\n",
    "pcaFit=pca.fit(all_first_three_wordVec)\n",
    "#pcaFit=pca.fit(np.vstack((pca_train_vec,pca_train_vec2,pca_train_vec3)))\n",
    "vectors2d_train = pcaFit.transform(train_arrays)\n",
    "vectors2d_test = pcaFit.transform(test_arrays)\n",
    "\n",
    "#pcaFit2=pca.fit(pca_train_vec2)\n",
    "vectors2d_train2 = pcaFit.transform(train_arrays2)\n",
    "vectors2d_test2 = pcaFit.transform(test_arrays2)\n",
    "\n",
    "#pcaFit3=pca.fit(pca_train_vec3)\n",
    "vectors2d_train3 = pcaFit.transform(train_arrays3)\n",
    "vectors2d_test3 = pcaFit.transform(test_arrays3)\n",
    "\n",
    "#vectors2d_train4 = pcaFit.transform(train_arrays4)\n",
    "#vectors2d_test4 = pcaFit.transform(test_arrays4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3493,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectors2d_train=np.hstack((vectors2d_train,vectors2d_train2,vectors2d_train3))\n",
    "vectors2d_test=np.hstack((vectors2d_test,vectors2d_test2,vectors2d_test3))\n",
    "\n",
    "#With length info\n",
    "#vectors2d_train=np.hstack((vectors2d_train,vectors2d_train2,vectors2d_train3,vectors2d_train4,len_train))\n",
    "#vectors2d_test=np.hstack((vectors2d_test,vectors2d_test2,vectors2d_test3,vectors2d_test4,len_test))\n",
    "\n",
    "#With length info\n",
    "#vectors2d_train=np.hstack((vectors2d_train,vectors2d_train2,vectors2d_train3,len_train))\n",
    "#vectors2d_test=np.hstack((vectors2d_test,vectors2d_test2,vectors2d_test3,len_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3497,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 3497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#classifier = LogisticRegression(penalty='l2')\n",
    "classifier=RandomForestClassifier()\n",
    "classifier.fit(vectors2d_train, train_labels)\n",
    "#classifier.fit(TA, train_labels)\n",
    "#classifier.fit(train_arrays, train_labels)\n",
    "#classifier.fit(c1*train_arrays+c2*train_arrays2+c3*train_arrays3+c4*train_arrays4, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3498,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 3498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#classifier.score(TestA, test_labels)\n",
    "#classifier.score(c1*test_arrays+c2*test_arrays2+c3*test_arrays3+c4*test_arrays4, test_labels)\n",
    "classifier.score(vectors2d_test, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3499,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.43848656,  0.25901269,  0.30250075])"
      ]
     },
     "execution_count": 3499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> test on real questions </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3500,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.75721154])"
      ]
     },
     "execution_count": 3500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "prediction=[]\n",
    "for line_arr in qs_yn[-int(len(qs_yn)/2):]:\n",
    "    try:\n",
    "        #prediction.append(classifier.predict(c1*QmodelB[line_arr[0]]+c2*QmodelB[line_arr[1]]+c3*QmodelB[line_arr[2]]\\\n",
    "        #                                    +c4*QmodelB[line_arr[3]]))\n",
    "        # Use with PCA, each word->1 number through PCA\n",
    "        prediction.append(classifier.predict(np.hstack((pcaFit.transform(QmodelB[line_arr[0]]),\\\n",
    "                                            pcaFit.transform(QmodelB[line_arr[1]]),\\\n",
    "                                            pcaFit.transform(QmodelB[line_arr[2]]),\\\n",
    "                                            #pcaFit.transform(QmodelB[line_arr[3]]),\\\n",
    "                                            #np.array(len(line_arr)).reshape(1,1) \\\n",
    "                                            ))))\n",
    "        # Each word->vector of size 100, no PCA\n",
    "        #prediction.append(classifier.predict(np.hstack((QmodelB[line_arr[0]],\\\n",
    "        #                                    QmodelB[line_arr[1]],\\\n",
    "        #                                    QmodelB[line_arr[2]],\\\n",
    "        #                                    ))))\n",
    "    except KeyError:\n",
    "        prediction.append(1)\n",
    "sum(prediction)/len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.91071429])"
      ]
     },
     "execution_count": 3254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#recall\n",
    "sum(prediction)/(sum(prediction)+sum(prediction2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2665,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71"
      ]
     },
     "execution_count": 2665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#score of their algorythm\n",
    "sum(val=='yes/no' for val in Qyn_Type_data[-200:])/200\n",
    "#sum(val=='yes/no' for val in Qyn_Type_bow[-200:])/200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3501,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74846625766871167"
      ]
     },
     "execution_count": 3501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction2=[]\n",
    "for line_arr in qs_oe[-int(len(qs_oe)/2):]:\n",
    "    try:\n",
    "        #prediction2.append(classifier.predict(c1*QmodelB[line_arr[0]]+c2*QmodelB[line_arr[1]]+c3*QmodelB[line_arr[2]]\\\n",
    "        #                                    +c4*QmodelB[line_arr[3]]))\n",
    "        prediction2.append(classifier.predict(np.hstack((pcaFit.transform(QmodelB[line_arr[0]]),\\\n",
    "                                            pcaFit.transform(QmodelB[line_arr[1]]),\\\n",
    "                                            pcaFit.transform(QmodelB[line_arr[2]]),\\\n",
    "                                            #pcaFit.transform(QmodelB[line_arr[3]]),\\\n",
    "                                            #np.array(len(line_arr)).reshape(1,1)\\\n",
    "                                            ))))\n",
    "        #prediction.append(classifier.predict(np.hstack((QmodelB[line_arr[0]],\\\n",
    "        #                            QmodelB[line_arr[1]],\\\n",
    "        #                            QmodelB[line_arr[2]],\\\n",
    "        #                            ))))\n",
    "    except KeyError:\n",
    "        prediction2.append(1)\n",
    "sum([int(val==0) for val in prediction2])/len(prediction2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84"
      ]
     },
     "execution_count": 3264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#score of their algorythm\n",
    "#sum(val=='open-ended' for val in Qoe_Type_bow[-200:])/200\n",
    "sum(val=='open-ended' for val in Qoe_Type_data[-200:])/200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1543,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 1543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> save classifier </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['three_word_logreg_py2.pkl',\n",
       " 'three_word_logreg_py2.pkl_01.npy',\n",
       " 'three_word_logreg_py2.pkl_02.npy',\n",
       " 'three_word_logreg_py2.pkl_03.npy',\n",
       " 'three_word_logreg_py2.pkl_04.npy']"
      ]
     },
     "execution_count": 1212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(classifier, 'three_word_logreg_py2.pkl',protocol=2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Model on first word only </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3539,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fail\n"
     ]
    }
   ],
   "source": [
    "#use the length of qs_oe_sample to get a 50/50 sample\n",
    "#only look at questions with more than 3 word inputs\n",
    "#shuffle and take the first 1/2 for training\n",
    "qs_yn=[val for val in qs_yn if len(val)>0]\n",
    "shuffle(qs_yn)\n",
    "qs_yn_sample=qs_yn[:int(len(qs_yn)/2)]\n",
    "\n",
    "#shuffle and take the first 1/2 for training, can also do random sampling\n",
    "qs_oe=[val for val in qs_oe if len(val)>0]\n",
    "shuffle(qs_oe)\n",
    "qs_oe_sample=qs_oe[:int(len(qs_oe)/2)]\n",
    "\n",
    "all_yn_arrays=[]\n",
    "all_oe_arrays=[]\n",
    "all_yn_labels=[]\n",
    "all_oe_labels=[]\n",
    "\n",
    "all_yn_labels=[]\n",
    "all_oe_labels=[]\n",
    "len_yn=[]\n",
    "len_oe=[]\n",
    "\n",
    "#setup training for Y/N questions ... only tranin on the first word!\n",
    "for i in range(len(qs_oe_sample)):\n",
    "    if(qs_yn_sample[i]==[]):\n",
    "        qs_yn_sample[i]=['']\n",
    "    if (qs_yn_sample[i][0] in QmodelB):\n",
    "        #print(qs_yn_sample[i][2])\n",
    "        all_yn_arrays.append(QmodelB[qs_yn_sample[i][0]])\n",
    "        len_yn.append(len(qs_yn_sample[i]))\n",
    "        all_yn_labels.append(1)\n",
    "    else:\n",
    "        print('fail')\n",
    "\n",
    "#setup training for open-ended questions\n",
    "for ii in range(len(qs_oe_sample)):\n",
    "    if(qs_oe_sample[ii]==[]):\n",
    "        qs_oe_sample[ii]=['']\n",
    "    if (qs_oe_sample[ii][0] in QmodelB):\n",
    "        #print(qs_oe_sample[ii][2])\n",
    "        all_oe_arrays.append(QmodelB[qs_oe_sample[ii][0]])\n",
    "        len_oe.append(len(qs_oe_sample[ii]))\n",
    "        all_oe_labels.append(0)\n",
    "    else:\n",
    "        print('fail',qs_oe_sample[ii])\n",
    "    \n",
    "#get length\n",
    "Noe=len(all_oe_arrays)\n",
    "ratio=0.99\n",
    "N=int(Noe*ratio)\n",
    "\n",
    "len_yn=np.array(len_yn)\n",
    "len_oe=np.array(len_oe)\n",
    "\n",
    "train_arrays = np.vstack((all_yn_arrays[:N],all_oe_arrays[:N]))\n",
    "len_train=np.hstack((len_yn[:N],len_oe[:N]))\n",
    "len_train=len_train.reshape(len(len_train),1)\n",
    "train_labels = np.hstack((all_yn_labels[:N],all_oe_labels[:N]))\n",
    "\n",
    "\n",
    "test_arrays = np.vstack((all_yn_arrays[N:],all_oe_arrays[N:]))\n",
    "len_test=np.hstack((len_yn[N:],len_oe[N:]))\n",
    "len_test=len_test.reshape(len(len_test),1)\n",
    "test_labels = np.hstack((all_yn_labels[N:],all_oe_labels[N:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3540,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 3540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pca_train_vec=np.vstack((train_arrays,test_arrays))\n",
    "#pca = PCA(n_components=1, whiten=True)\n",
    "#pcaFit=pca.fit(pca_train_vec)\n",
    "#pcaFit=pca.fit(pca_train_vec)\n",
    "#vectors2d_train = pcaFit.transform(train_arrays)\n",
    "#vectors2d_test = pcaFit.transform(test_arrays)\n",
    "\n",
    "classifier = LogisticRegression(penalty='l2')\n",
    "#classifier=RandomForestClassifier()\n",
    "classifier.fit(train_arrays, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3541,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66666666666666663"
      ]
     },
     "execution_count": 3541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(test_arrays, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> test on real questions </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3542,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86616161616161613"
      ]
     },
     "execution_count": 3542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "prediction=[]\n",
    "for line_arr in qs_yn[-int(len(qs_yn)/2):]:\n",
    "    try:\n",
    "        #prediction.append(classifier.predict(c1*QmodelB[line_arr[0]]+c2*QmodelB[line_arr[1]]+c3*QmodelB[line_arr[2]]\\\n",
    "        #                                    +c4*QmodelB[line_arr[3]]))\n",
    "        #prediction.append(classifier.predict(pcaFit.transform(QmodelB[line_arr[0]])))\n",
    "        prediction.append(classifier.predict(QmodelB[line_arr[0]]))\n",
    "    except KeyError:\n",
    "        prediction.append(1)\n",
    "sum(prediction)/len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3543,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78169014084507038"
      ]
     },
     "execution_count": 3543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction2=[]\n",
    "for line_arr in qs_oe[-int(len(qs_oe)/2):]:\n",
    "    try:\n",
    "        #prediction2.append(classifier.predict(c1*QmodelB[line_arr[0]]+c2*QmodelB[line_arr[1]]+c3*QmodelB[line_arr[2]]\\\n",
    "        #                                    +c4*QmodelB[line_arr[3]]))\n",
    "        #prediction2.append(classifier.predict(pcaFit.transform(QmodelB[line_arr[0]])))\n",
    "        prediction2.append(classifier.predict(QmodelB[line_arr[0]]))\n",
    "    except KeyError:\n",
    "        prediction2.append(1)\n",
    "        \n",
    "sum([int(val==0) for val in prediction2])/len(prediction2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3544,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ubuntu/TallLabs/models/first_word_logreg_py.pkl',\n",
       " '/home/ubuntu/TallLabs/models/first_word_logreg_py.pkl_01.npy',\n",
       " '/home/ubuntu/TallLabs/models/first_word_logreg_py.pkl_02.npy',\n",
       " '/home/ubuntu/TallLabs/models/first_word_logreg_py.pkl_03.npy',\n",
       " '/home/ubuntu/TallLabs/models/first_word_logreg_py.pkl_04.npy']"
      ]
     },
     "execution_count": 3544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(classifier, '/home/ubuntu/TallLabs/models/first_word_logreg_py2.pkl',protocol=2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> backup of model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use the length of qs_oe_sample to get a 50/50 sample\n",
    "#only look at questions with more than 3 word inputs\n",
    "#shuffle and take the first 1/2 for training\n",
    "qs_yn=[val for val in qs_yn if len(val)>3]\n",
    "shuffle(qs_yn)\n",
    "qs_yn_sample=qs_yn[:int(len(qs_yn)/2)]\n",
    "\n",
    "#shuffle and take the first 1/2 for training, can also do random sampling\n",
    "qs_oe=[val for val in qs_oe if len(val)>3]\n",
    "shuffle(qs_oe)\n",
    "qs_oe_sample=qs_oe[:int(len(qs_oe)/2)]\n",
    "\n",
    "'''\n",
    "all_yn_arrays = np.zeros((len(qs_oe_sample), 100))\n",
    "all_oe_arrays = np.zeros((len(qs_oe_sample), 100))\n",
    "all_yn_labels = np.zeros(len(qs_oe_sample))\n",
    "all_oe_labels = np.zeros(len(qs_oe_sample))\n",
    "\n",
    "all_yn_arrays2 = np.zeros((len(qs_oe_sample), 100))\n",
    "all_oe_arrays2 = np.zeros((len(qs_oe_sample), 100))\n",
    "all_yn_labels2 = np.zeros(len(qs_oe_sample))\n",
    "all_oe_labels2 = np.zeros(len(qs_oe_sample))\n",
    "\n",
    "all_yn_arrays3 = np.zeros((len(qs_oe_sample), 100))\n",
    "all_oe_arrays3 = np.zeros((len(qs_oe_sample), 100))\n",
    "all_yn_labels3 = np.zeros(len(qs_oe_sample))\n",
    "all_oe_labels3 = np.zeros(len(qs_oe_sample))\n",
    "\n",
    "all_yn_arrays4 = np.zeros((len(qs_oe_sample), 100))\n",
    "all_oe_arrays4 = np.zeros((len(qs_oe_sample), 100))\n",
    "all_yn_labels4 = np.zeros(len(qs_oe_sample))\n",
    "all_oe_labels4 = np.zeros(len(qs_oe_sample))\n",
    "'''\n",
    "all_yn_arrays=[]\n",
    "all_oe_arrays=[]\n",
    "all_yn_labels=[]\n",
    "all_oe_labels=[]\n",
    "\n",
    "all_yn_arrays2=[]\n",
    "all_oe_arrays2=[]\n",
    "all_yn_labels2=[]\n",
    "all_oe_labels2=[]\n",
    "\n",
    "all_yn_arrays3=[]\n",
    "all_oe_arrays3=[]\n",
    "all_yn_labels3=[]\n",
    "all_oe_labels3=[]\n",
    "\n",
    "len_yn=[]\n",
    "len_oe=[]\n",
    "\n",
    "#setup training for Y/N questions ... only tranin on the first word!\n",
    "for i in range(len(qs_oe_sample)):\n",
    "    if(qs_yn_sample[i]==[]):\n",
    "        qs_yn_sample[i]=['']\n",
    "    try:\n",
    "        all_yn_arrays[i] = QmodelB[qs_yn_sample[i][0]]\n",
    "        all_yn_arrays2[i] = QmodelB[qs_yn_sample[i][1]]\n",
    "        all_yn_arrays3[i] = QmodelB[qs_yn_sample[i][2]]\n",
    "        #all_yn_arrays4[i] = QmodelB[qs_yn_sample[i][3]]\n",
    "        len_yn.append(len(qs_yn_sample[i]))\n",
    "    except KeyError:\n",
    "        pass\n",
    "        all_yn_arrays[i] = all_yn_arrays[i-1]\n",
    "        all_yn_arrays2[i] = all_yn_arrays2[i-1]\n",
    "        all_yn_arrays3[i] = all_yn_arrays3[i-1]\n",
    "        #all_yn_arrays4[i] = all_yn_arrays4[i-1]\n",
    "        len_yn.append(len(qs_yn_sample[i]))\n",
    "        #print('fail')\n",
    "    all_yn_labels[i] = 1\n",
    "\n",
    "#setup training for open-ended questions\n",
    "for ii in range(len(qs_oe_sample)):\n",
    "    if(qs_oe_sample[ii]==[]):\n",
    "        qs_oe_sample[ii]=['']\n",
    "    try:\n",
    "        all_oe_arrays[ii] = QmodelB[qs_oe_sample[ii][0]]\n",
    "        all_oe_arrays2[ii] = QmodelB[qs_oe_sample[ii][1]]\n",
    "        all_oe_arrays3[ii] = QmodelB[qs_oe_sample[ii][2]]\n",
    "        #all_oe_arrays4[ii] = QmodelB[qs_oe_sample[ii][3]]\n",
    "        len_oe.append(len(qs_oe_sample[ii]))\n",
    "    except KeyError:\n",
    "        pass\n",
    "        all_oe_arrays[ii]=all_oe_arrays[ii-1]\n",
    "        all_oe_arrays2[ii]=all_oe_arrays2[ii-1]\n",
    "        all_oe_arrays3[ii]=all_oe_arrays3[ii-1]\n",
    "        #all_oe_arrays4[ii]=all_oe_arrays4[ii-1]\n",
    "        len_oe.append(len(qs_oe_sample[ii]))\n",
    "        print('fail',qs_oe_sample[ii])\n",
    "    all_oe_labels[ii] = 0\n",
    "    \n",
    "#get length\n",
    "ratio=0.7\n",
    "N=int(Nyn*ratio)\n",
    "\n",
    "len_yn=np.array(len_yn)\n",
    "len_oe=np.array(len_oe)\n",
    "\n",
    "train_arrays = np.vstack((all_yn_arrays[:N],all_oe_arrays[:N]))\n",
    "len_train=np.hstack((len_yn[:N],len_oe[:N]))\n",
    "len_train=len_train.reshape(len(len_train),1)\n",
    "train_labels = np.hstack((all_yn_labels[:N],all_oe_labels[:N]))\n",
    "train_arrays2 = np.vstack((all_yn_arrays2[:N],all_oe_arrays2[:N]))\n",
    "train_arrays3 = np.vstack((all_yn_arrays3[:N],all_oe_arrays3[:N]))\n",
    "train_arrays4 = np.vstack((all_yn_arrays4[:N],all_oe_arrays4[:N]))\n",
    "\n",
    "\n",
    "test_arrays = np.vstack((all_yn_arrays[N:],all_oe_arrays[N:]))\n",
    "len_test=np.hstack((len_yn[N:],len_oe[N:]))\n",
    "len_test=len_test.reshape(len(len_test),1)\n",
    "test_labels = np.hstack((all_yn_labels[N:],all_oe_labels[N:]))\n",
    "test_arrays2 = np.vstack((all_yn_arrays2[N:],all_oe_arrays2[N:]))\n",
    "test_arrays3 = np.vstack((all_yn_arrays3[N:],all_oe_arrays3[N:]))\n",
    "test_arrays4 = np.vstack((all_yn_arrays4[N:],all_oe_arrays4[N:]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
